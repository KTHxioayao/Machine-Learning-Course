#assignment 2
#task1

#install.packages("caret")
library(caret)

data <- read.csv("parkinsons.csv")
n <- dim(data)[1]
set.seed(12345)
id <- sample(1:n, floor(n * 0.6))
train_data = data[id, ]
test_data = data[-id, ]

scaler = preProcess(train_data)
train_data_scaled = predict(scaler, train_data)
test_data_scaled = predict(scaler, test_data)



#task2
model <- lm(motor_UPDRS ~ . - subject. - age - sex - test_time - total_UPDRS ,
            train_data_scaled)
train_prediction <- predict(model, train_data_scaled)
train_mse <- mean((train_prediction - train_data_scaled$motor_UPDRS) ^ 2)

test_prediction <- predict(model, test_data_scaled)
test_mse <- mean((test_prediction - test_data_scaled$motor_UPDRS) ^ 2)
#A smaller p-value means a larger contribution to the model.
#Specifically, Jitter.Abs., Shimmer.APQ5,
#Shimmer.APQ11, NHR, HNR, DFA and PPE contribute significantly to the model.

#task3
#a
logLikelihood <- function(theta, sigma, x, y) {
  n <- length(y)
  predictions <- x %*% theta
  residuals <- y - predictions
  log_likelihood <- -n / 2 * log(2 * pi * sigma ^ 2) - 1 / (2 * sigma ^
                                                              2) * sum(residuals ^ 2)
  return(as.numeric(log_likelihood))
}
#b
ridge <- function(theta, sigma, lambda, x, y) {
  log_likelihood <- logLikelihood(theta, sigma, x, y)
  ridge_penalty <- lambda * sum(theta ^ 2)# Ridge penalty: Î»â€–Î¸â€–Â²
  return(-log_likelihood + ridge_penalty)
}


#c
#åœ¨ Ridge å›å½’ä¼˜åŒ–ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›åŒæ—¶æ‰¾åˆ°æœ€ä½³çš„ï¼š
#Î¸ï¼šæ¨¡å‹çš„ç³»æ•°å‘é‡ï¼ˆä¸ç‰¹å¾ ğ‘‹X çš„ç»´åº¦ä¸€è‡´ï¼‰ã€‚
#Ïƒï¼šæ¨¡å‹çš„æ ‡å‡†å·®ï¼ˆæ ‡é‡ï¼‰ã€‚ç”±äº optim() å‡½æ•°åªèƒ½æ¥å—å•ä¸ªå‘é‡ä½œä¸ºå‚æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°† Î¸ å’Œ
#ğœåˆå¹¶åˆ°ä¸€ä¸ªå‘é‡ params ä¸­è¿›è¡Œä¼˜åŒ–ã€‚ä¼˜åŒ–å®Œæˆåï¼Œå†é€šè¿‡åˆ†å‰²è¿™ä¸ªå‘é‡æ¥æå– Î¸ å’Œ Ïƒã€‚
#å‚æ•°åˆ†å‰²é€»è¾‘params[1:p]: æå–å‰ p ä¸ªå…ƒç´ ï¼Œè¡¨ç¤º Î¸ï¼Œè¿™é‡Œ p æ˜¯ç‰¹å¾çš„ä¸ªæ•°ã€‚
#params[p + 1]: æå–æœ€åä¸€ä¸ªå…ƒç´ ï¼Œè¡¨ç¤ºÏƒã€‚

ridgeopt <- function(lambda, x, y) {
  n <- ncol(x)# ç‰¹å¾çš„æ•°é‡
  init_params <- c(rep(0, n), 1)# åˆå§‹åŒ–Î¸ä¸º0å‘é‡ï¼ŒÏƒä¸º1,ä¸åŒåˆå§‹å€¼ä¼šç»™ä¸€æ ·çš„ç»“æœï¼Œä½†æ˜¯éœ€è¦ä¿è¯n+1ä¸ªå‚æ•°

  # Objective function for optimization (negative penalized log-likelihood)
  ridge_obj <- function(params) {
    theta <- params[1:n]
    sigma <- params[n + 1]
    return(ridge(theta, sigma, lambda, x, y))
  }
  # Optimization using optim() with method = "BFGS"
  #init_params is usde as the initial values for the optimization (fn)
  opt <- optim(par = init_params, fn = ridge_obj, method = "BFGS")

  # Extract optimized theta and sigma
  theta_opt <- opt$par[1:n]
  sigma_opt <- opt$par[n + 1]
  return(list(theta = theta_opt, sigma = sigma_opt))
}

#d è®¡ç®—Ridgeæ¨¡å‹çš„è‡ªç”±åº¦
freedom_degree <- function(lambda, x) {
  #n <- nrow(x)

  xT <- t(x) %*% x
  p <- ncol(x)# ç‰¹å¾æ•°é‡
  I <- diag(p)# å•ä½çŸ©é˜µ
  xtx <- t(x) %*% x# Xáµ€X
  ridge_matrix <- xtx + lambda * I  # Ridge matrix: Xáµ€X + Î»I
  hat_matrix <- x %*% solve(ridge_matrix) %*% t(x)  # Hat matrix
  df <- sum(diag(hat_matrix))  # Trace of the hat matrix # HatçŸ©é˜µçš„è¿¹å³ä¸ºè‡ªç”±åº¦
  return(as.numeric(df))
}

# Generate example data
set.seed(123)
x <- matrix(rnorm(100), nrow = 20, ncol = 5)  # 20 samples, 5 predictors
y <- rnorm(20)  # Response vector
lambda <- 1  # Ridge penalty parameter

# a. Compute log-likelihood
theta <- runif(5)
sigma <- 1.5
ll <- logLikelihood(theta, sigma, x, y)
print(ll)

# b. Compute Ridge penalized log-likelihood
ridge_ll <- ridge(theta, sigma, lambda, x, y)
print(ridge_ll)

# c. Optimize Ridge regression
opt_result <- ridgeopt(lambda, x, y)
print(opt_result)

# d. Compute degrees of freedom
df <- freedom_degree(lambda, x)
print(df)




#task4
train_data2 <- as.matrix(train_data_scaled[7:length(train_data_scaled)])
test_data2 <- as.matrix(test_data_scaled[7:length(test_data_scaled)])
train_value<- train_data_scaled$motor_UPDRS
test_value<- test_data_scaled$motor_UPDRS

lambda_values <- c(1, 100, 1000)

train_mse2 <- c()
test_mse2 <- c()
df <- c()
theta_value <- list()
# éå†ä¸åŒçš„Î»å€¼ï¼Œè®­ç»ƒæ¨¡å‹å¹¶è®¡ç®—æŒ‡æ ‡

for (i in 1:length(lambda_values)) {
  lambda <- lambda_values[i]

  ridgemodel <- ridgeopt(lambda, train_data2, train_value) # æ¨¡å‹
  theta_value[[i]]  <- ridgemodel$theta# å­˜å‚¨Î¸

  # è®¡ç®—è®­ç»ƒé›†çš„é¢„æµ‹å€¼å’ŒMSE
  train_predictions <- train_data2 %*%  theta_value[[i]]
  train_mse2[i] <- mean((train_value - train_predictions) ^ 2)

  # è®¡ç®—æµ‹è¯•é›†çš„é¢„æµ‹å€¼å’ŒMSE
  test_predictions <- test_data2 %*%  theta_value[[i]]
  test_mse2[i] <- mean((test_value - test_predictions) ^ 2)

  df[i] <- freedom_degree(lambda, train_data2)

  result <- list(
    train_mse2 = train_mse2,
    test_mse2 = test_mse2,
    df = df,
    theta_value = theta_value
  )

}
print(result)
